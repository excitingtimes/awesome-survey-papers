# awesome-survey-papers

A currated list of **Survey Papers** in **Machine Learning**. Feel free to add your favourite **literature reviews** or **survey blog posts** by raising a *pull request*. 

For any questions, suggestions or typos, feel free to reach out to me at [hugo.sonnery@mail.mcgill.ca](mailto:hugo.sonnery@mail.mcgill.ca).

## Contents

- [awesome-survey-papers](#awesome-survey-papers)
  - [Contents](#contents)
  - [OOD, generalization and compositionality](#ood-generalization-and-compositionality)
  - [Continual learning](#continual-learning)
  - [Training dynamics and Optimization](#training-dynamics-and-optimization)
  - [Efficient Transformers](#efficient-transformers)
  - [Graph neural networks](#graph-neural-networks)
  - [Generative modeling](#generative-modeling)
  - [Self-supervised learning](#self-supervised-learning)
  - [Natural Language Processing](#natural-language-processing)
  - [Music Generation](#music-generation)
  - [Reinforcement Learning](#reinforcement-learning)
  - [Neural AI](#neural-ai)
  - [Computer Vision](#computer-vision)
  - [3D, Point Clouds and Meshes](#3d-point-clouds-and-meshes)
  - [Biology](#biology)
  - [Physics](#physics)



## OOD, generalization and compositionality
[Back to Top](#contents)

* [A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges](https://arxiv.org/abs/2110.14051v1) (*Mohammadreza Salehi et al*, **October 2021**)
* [A Survey of Generalisation in Deep Reinforcement Learning](https://arxiv.org/abs/2111.09794) (*Robert Kirk et al*, **November 2021**)




## Continual learning
[Back to Top](#contents)

* [A curated, up-to-date list of scaling law papers](https://sites.google.com/site/irinarish/scaling-laws) (cc. Prof. *Irina Rish*)
* [A curated, up-to-date list of OoD generalization papers](https://sites.google.com/site/irinarish/ood_generalization) (cc. Prof. *Irina Rish*)
* [A curated, up-to-date list of continual learning papers](https://sites.google.com/site/irinarish/continuallearning/resources) (cc. Prof. *Irina Rish*)
* [Continual lifelong learning with neural networks : a review](https://www.sciencedirect.com/science/article/pii/S0893608019300231) (*German I. Parisi et al*, **May 2019**)



## Training dynamics and Optimization
[Back to Top](#contents)

* [Evolution strategies](https://lilianweng.github.io/lil-log/2019/09/05/evolution-strategies.html) (*Lilian Weng*'s blog, **September 2019**)
* [A survey of optimization methods from a machine learning perspective](https://arxiv.org/abs/1906.06821) (*Sun et al*, **October 2019**)
* [A Review of Uncertainty Quantification in Deep Learning: Techniques, Applications and Challenges](https://arxiv.org/abs/2011.06225v3) (*Mouloud Abdar et al*, **November 2020**)




## Efficient Transformers
[Back to Top](#contents)

* [A Survey of Transformers](https://arxiv.org/abs/2106.04554) (*Lin et al*, **June 2021**)
* [Efficient Transformers : a survey](https://arxiv.org/abs/2009.06732) (*Tay et al*, **September 2020**)
* [A practical survey on Faster and Lighter transformers](https://arxiv.org/abs/2103.14636) (*Fournier et al*, **March 2021**)
* [A primer on Transformers, Attention, Tokenization and Positional Encodings](https://msank00.github.io/blog/2020/03/15/blog_605_Survey_attention) (*Sankarshan Mridha*'s blog)
* [An attentive survey of Attention models](https://arxiv.org/abs/1904.02874) (*Chaudhari et al*, **April 2019**)
* [Attention ? Attention !](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) (*Lilian Weng*'s blog, **June 2018**)


## Graph neural networks
[Back to Top](#contents)

* [Combinatorial optimization and reasoning with GNNs](https://arxiv.org/abs/2102.09544) (*Cappart et al*, **February 2021**)
* [Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges](https://arxiv.org/abs/2104.13478) (*Bronstein et al*, **April 2021**)
* [Attention Models in Graphs: A Survey](https://dl.acm.org/doi/abs/10.1145/3363574) (*John Boaz Lee et al*, **November 2019**)



## Generative modeling
[Back to Top](#contents)

* [Generative adversarial networks in time series: A survey and taxonomy](https://arxiv.com/abs/2107.11098) (*Brophy et al*, **July 2021**)

* [Diffusion models](https://lilianweng.github.io/lil-log/2021/07/11/diffusion-models.html) (*Lilian Weng*'s blog, **July 2021**)



## Self-supervised learning
[Back to Top](#contents)

* [Contrastive representation learning](https://lilianweng.github.io/lil-log/2021/05/31/contrastive-representation-learning.html) (*Lilian Weng*'s blog, **May 2021**)
* [Contrastive representation learning : a framework and review](https://arxiv.org/abs/2010.05113) (*Le-Khac et al*, **October 2020**)
* [Self-supervised learning : generative or contrastive](https://arxiv.org/abs/2006.08218) (*Liu et al*, **June 2020**)
* [A survey on contrastive self-supervised learning](https://arxiv.org/abs/2011.00362) (*Jaiswal et al*, **October 2020**)
* [Self-supervised representation learning](https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html) (*Lilian Weng*'s blog, **November 2019**)



## Natural Language Processing
[Back to Top](#contents)

* [Controllable neural text generation](https://lilianweng.github.io/lil-log/2021/01/02/controllable-neural-text-generation.html) (*Lilian Weng*'s blog, **January 2021**)
* [Emergent Multi-Agent Communication in the Deep Learning Era](https://arxiv.org/abs/2006.02419) (*Lazaridou et al*, **June 2020**)
* [Which BERT : a survey organizing contextualized encoders](https://arxiv.org/abs/2010.00854) (*Xia et al*, **October 2020**)
* [Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey](https://arxiv.org/abs/2111.01243) (*Bonan Min et al*, **November 2021**)


## Music Generation
[Back to Top](#contents)

* [Music Composition with Deep Learning: A Review](https://arxiv.org/abs/2108.12290v2) (*Hernandez-Olival et al*, **August 2021**)
* [A comprehensive survey on Deep Music Generation](https://arxiv.org/abs/2011.06801) (*Ji et al*, **November 2020**)
* [Deep Learning techniques for Music Generation](https://hal.archives-ouvertes.fr/hal-01660772) (*Briot et al*, **June 2019**)



## Reinforcement Learning
[Back to Top](#contents)

* [Multi-agent Deep Reinforcement Learning : a survey](https://link.springer.com/article/10.1007/s10462-021-09996-w) (*Gronauer et al*, **April 2021**)
* [Exploration strategies in Deep Reinforcement Learning](https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html) (*Lilian Weng*'s blog, **June 2020**)
* [Curriculum for Reinforcement Learning](https://lilianweng.github.io/lil-log/2020/01/29/curriculum-for-reinforcement-learning.html) (*Lilian Weng*'s blog, **January 2020**)
* [Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey](https://arxiv.org/abs/2009.13303) (*Wenshuai Zhao et al*, **September 2020**)



## Neural AI
[Back to Top](#contents)

* [Neuroscience-Inspired Artificial Intelligence (Neuron Review)](https://www.cell.com/neuron/pdf/S0896-6273(17)30509-3.pdf) (*Demis Hassabis et al*, **July 2017**)
* [An introduction to theories of learning](https://www.taylorfrancis.com/books/mono/10.4324/9781003014447/introduction-theories-learning-matthew-olson-julio-ramirez) (*Matthew H. Olson et al*, **February 2020**)
* [Cognitive computational neuroscience : a review](https://www.nature.com/articles/s41593-018-0210-5) (*Nikolaus Kriegeskorte et al*, **August 2018**)
* [Bridging Biological and Artificial Neural Networks with Emerging Neuromorphic Devices: Fundamentals, Progress, and Challenges](https://onlinelibrary.wiley.com/doi/abs/10.1002/adma.201902761) (*Jianshi Tang et al*, **September 2019**)



## Computer Vision
[Back to Top](#contents)

* [Advances in adversarial attacks and defenses in computer vision: A survey](https://arxiv.org/abs/2108.00401) (*Naveed Akhtar et al*, **August 2021**)
* [Attention Mechanisms in Computer Vision: A Survey](https://arxiv.org/abs/2111.07624v1) (*Meng-Hao Guo et al*, **November 2021**)
* [Advances in Neural Rendering](https://arxiv.org/abs/2111.05849v1) (*Ayush Tewari et al*, **November 2021**)
* [Are we ready for a new paradigm shift? A Survey on Visual Deep MLP](https://arxiv.org/abs/2111.04060v3) (*Ruiyang Liu et al*, **November 2021**)
* [A Survey of Visual Transformers](https://arxiv.org/abs/2111.06091) (*Yang Liu et al*, **November 2021**)



## 3D, Point Clouds and Meshes
[Back to Top](#contents)

* [Deep Learning for 3D Point Clouds: A Survey](https://arxiv.org/abs/1912.12033) (*Yulan Guo et al*, **December 2019**)




## Biology
[Back to Top](#contents)

* [Medical Visual Question Answering : a survey](https://arxiv.org/abs/2111.10056) (*Zhihong Lin et al*, **November 2021**)



## Physics
[Back to Top](#contents)

* [Physics-based deep learning](https://arxiv.org/abs/2109.05237) (*Thuerey et al*, **September 2021**)
* [Physics-Guided Deep Learning for Dynamical Systems: A Survey](https://arxiv.orghttps://arxiv.org/pdf/2107.01272.pdf) (*Wang et al*, **July 2021**)